# zData Ambari Stack with Hawq Service

If you haven't heard about the [Apache Ambari project](http://ambari.apache.org/ "Apache Ambari Project") yet and you work with big data, you're missing out. Apache Ambari is a framework able to provision, manage, and monitor Apache Hadoop deployments. We will assume you have some familiarity with Ambari features and terminology. If you don't, don't sweat it! You can get caught up to speed from our [previous blog post](http://www.zdatainc.com/2014/11/apache-ambari-overview/).

We will be going over a project that we've been working on for the past few weeks: zData's Ambari Stack. The zData Ambari Stack defines a custom Ambari stack version that provides [PivotalÂ Hawq](http://pivotal.io/big-data/pivotal-hawq) as a service. This means installing HAWQ has never been easier! You can quickly install HAWQ on a cluster to try out, benchmark, use, compare, or anything else. We'll be covering how we set up our stack definition, some design decisions we made, some issues we ran into, and finally how to download the zData stack and try it out yourself.

## What's a Stack Again?

An Ambari stack is a collection of services and how to install those services. Each service is defined with certain life-cycle commands (for example the life-cycle commands for a master are start, stop, install, configure, and status) and configuration information so Ambari can provision, manage, and monitor one or more services on a cluster. A stack also contains repository information used to find packages for installation, and meta information which describes that stack itself.

## About the zData Stack

The zData stack was created as a proof of concept, with the goal of giving the open source community an easy way to install HAWQ using Ambari. We wanted to start out with getting Pivotal HAWQ working in Ambari which has HDFS as a prerequisite. Instead of integrating [Pivotal HD](http://pivotal.io/big-data/pivotal-hd) into a new service and having that work first, we decided to inherit off of [Hortonwork's](http://hortonworks.com/hdp/) HDP 2.0.6 stack. Why? It works, it's tested, and it allowed us to get HAWQ working on HDFS as fast as possible. Stack inheritance allows the new stack version to install all its parent services and define new ones.  One of this project's longterm goals is to add Pivotal's HDFS / Hadoop distribution to the zData stack and define it as a new service.

Because we're currently inheriting from HDP 2.0.6, the zData stack is in actuality a stack version. It isn't possible to inherit services between stacks in Ambari, so our decision to use some of HortonWork's services requires our stack version be a part of their stack. We define our stack version and copy it over to the HDP directory in the Ambari Server's /var/lib/ambari-server/resources/stacks directory so inheritance works properly.

## HAWQ as an Ambari Service

To integrate HAWQ into Ambari, we closely followed [manual HAWQ installation steps](http://pivotalhd.docs.pivotal.io/doc/2100/webhelp/index.html#topics/InstallingHAWQ.html) provided by Pivotal, tweaking them where necessary to be more Ambari friendly. To actually install HAWQ, you need to have downloaded Pivotal HD 2.1.0 and Pivotal HAWQ 1.2.1.0. These are usually distributed by Pivotal through their website and [can be downloaded from their website](https://network.pivotal.io/products/pivotal-hd). These distributions contain a number of rpms in an archive. To tie this into Ambari, we create a local repository server on the Ambari server that containes all of the packages. We simply extract both downloads into /var/www/html/phd, and run [createrepo](http://createrepo.baseurl.org/).  This is handled by the `build/setup-repo.sh` command when using vagrant. This allows us to be able to run `yum install hawq` and have yum take care of the dependencies. This simplifies Ambari's role in the actual installation of HAWQ quite a bit.

Actaully creating the service definitions for Ambari requires quite a bit of custom Python; all of the service commands are written in Python. A great place to learn how to define a new service from scratch is from [this page in Ambari Wiki](https://cwiki.apache.org/confluence/pages/viewpage.action?pageId=38571133). It was a great help! Most of the steps in the installation guide for HAWQ are replicated in the installation command for the service.  
For instance, the gpssh command is used to distribute keys to all of the servers in a cluster, allowing passwordless logins between them for a specific user.  Instead of using the command 'gpssh' to accomplish this for the root user, we allow the Ambari registering step to take care of it (though we still use the command to distribute keys for the hawq user). Another example would be instead of copying the rpms that HAWQ requires to every server in the cluster, we instead have every server connect to the local repository defined on the Ambari server, which saves time and reduces complexity.

One significant part of any service is making it configurable. Ambari simplifies making services configurable by using XML files that contain a description of each configuration variable.  Every one of these variables is then made available in service's life-cycle commands. Almost every setting that goes into the gpinitsystem_config file was made configurable via Ambari. While adding a new service, or modifying an existing one, Ambari uses these XML files to create a user interface to modify the configurations.  The user can make changes or keep the defaults and save the various values. We set some default values but recommend that everyone review them. Finally, all of the configurations are written to a gpinitsystem_config file and used for the `gpinitsystem` command, which is all taken care of by our custom life-cycle commands for HAWQ.
