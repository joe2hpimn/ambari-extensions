# zData Ambari Stack with Hawq Service

If you haven't heard about the [Apache Ambari project](http://ambari.apache.org/ "Apache Ambari Project") yet and you're in the big data world, you're missing out. Period. Apache Ambari is a framework to provision, manager, and monitor Apache Hadoop frameworks. This article will assume you have some basic familiarity with Ambari features and terminology. If you aren't, don't sweat it! You can get caught up to speed from our previous blog post [here](http://www.zdatainc.com/2014/11/apache-ambari-overview/).

This article will go over a project that we've been working on for the past few weeks to define a custom Ambari stack that has [Pivotal Hawq](http://pivotal.io/big-data/pivotal-hawq "Pivotal Hawq") as a service. This means that using our custom stack definition with Ambari, you could install Hawq on a cluster and try it out, benchmark it, use it, compare it, etc. We'll cover how we set up our stack definition, some design decisions we made, some issues we ran into, and finally how to download the zData stack and try it out.

## What's a Stack Again?

An Ambari stack is a collection of services and how to install those services. Each service is defined with lifecycle commands (start, stop, install, configure, status) and configuration information so Ambari can provision, manage, and monitor one or more services on a cluster. A stack also contains repository information to describe where Ambari can get the rpms or packages from and meta information to define the stack itself and give it a version.

## About the zData Stack

Our goal with the zData stack is to give the open source community an easy way to install some new services using Ambari. We wanted to start out with getting Pivotal Hawq working in Ambari which requires that HDFS service is already installed on the cluster. Instead of integrating [Pivotal HD](http://pivotal.io/big-data/pivotal-hd "Pivotal HD") into a new service and getting that to work first, we decided to inherit off of [Hortonwork's](http://hortonworks.com/hdp/ "Hortonwork") HDP 2.0.6 stack. Why? It works, it's tested, and it allowed us to get Hawq working on HDFS as fast as possible. Stack inheritance allows the new stack version to install all the parent services and then define new ones too.

In the future, we may try to add Pivotal's HDFS / Hadoop distribution to the zData stack and define it as a new service if it makes sense. Because it's inheriting off of HDP 2.0.6, the zData stack is really a new stack version. In Ambari, you can't define a brand new stack in a different directory and inherit from another stack definition in another. We define our stack version and copy it over to the HDP directory in the Ambari Server's /var/lib/ambari-server/resources/stacks directory so inheritance works properly.

## Hawq as an Ambari Service

To integrate Hawq into Ambari, we modified the steps followed the [manual Hawq installation steps](http://pivotalhd.docs.pivotal.io/doc/2100/webhelp/index.html#topics/InstallingHAWQ.html) provided by Pivotal and tweaked a lot to make everything Ambari friendly. To actually install Hawq, you need to have downloaded Pivotal HD 2.1.0 and Pivotal HAWQ 1.2.1.0. These are usually distributed by Pivotal through their website and can be downloaded [here](https://network.pivotal.io/products/pivotal-hd). These distributions are usually a bunch of rpm files zipped up nicely. To tie this into Ambari, we created a local repository server on the Ambari server that contained all of the packages. We simply unzipped both downloads into /var/www/html/phd, cd'ed into that directory, and ran create repo. This allowed us to be able to run `yum install hawq` and have yum take care of the dependencies. Ultimately, setting up the repository is so that Ambari can easily install required packages during the installation phase of a service and know which repo definition use.

Aside from setting up a local repository for the required software, we had to write our own Python scripts and configurations. All of the service commands are written in Python.  A great place to learn how to define a new service from scratch is from the Ambari Wiki, specifically [this](https://cwiki.apache.org/confluence/pages/viewpage.action?pageId=38571133) post. It was a great help! Most of the steps the installation guide for Hawq is in our Python but done a little differently. For instance, instead using the command 'gpssh' to create the hawq user everywhere, we can just have the Ambari segment installation do it for us. Also instead of copying rpms around that hawq requires, the local repository takes care of that for us so Ambari can install from that. There's a lot of minor nuances like that.
One significant piece to writing any service is how to make it configurable. Ambari does this with xml files that contains a name, value, and description for each configuration variable. Almost every setting that goes into the gpinitsystem_config file was taken out and put in an xml configuration file. During installation, Ambari reads that xml file with the default values and displays it to the user doing the installation. The user can make changes or keep the defaults and continue the installation. We set some default values but recommend that everyone review them. Finally, all those settings are written to a gpinitsystem_config file and used for the `gpinitsystem` command. All of this is done with our custom lifecycle commands for Hawq, and using certain Python functions and classes that Ambari comes with by default to make some common tasks much easier.
